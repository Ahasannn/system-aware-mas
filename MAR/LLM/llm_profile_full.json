{
  "global_settings": {
    "default_max_model_len": 16384,
    "default_max_output_tokens": 4096,
    "gpu_type": "NVIDIA B200",
    "total_gpus": 2
  },
  "models": [
    {
      "Name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
      "Description": "The 'Reasoning Engine'. A 32B model distilled from DeepSeek-R1. It utilizes long-chain Chain-of-Thought (CoT) to achieve state-of-the-art performance in logic and mathematics. It requires significant output token budget for internal reasoning. [Benchmarks: MATH 90.6%, AIME 72.0%, GPQA 62.1%].",
      "MaxModelLen": 16384,
      "MaxOutputTokens": 4096,
      "vllm_config": {
        "port": 8001,
        "gpu_device": 0,
        "gpu_memory_utilization": 0.50,
        "tensor_parallel_size": 1,
        "dtype": "bfloat16",
        "trust_remote_code": true,
        "enforce_eager": false
      }
    },
    {
      "Name": "mistralai/Mistral-Small-24B-Instruct-2501",
      "Description": "The 'Generalist'. Mistral Small 3 (Tekken) is a high-density model that balances speed and intelligence. It serves as the primary chat and instruction-following agent, outperforming larger previous-gen models in general knowledge. [Benchmarks: MMLU 81.0%, HumanEval 82.1%, MMLU-Pro 48.3%].",
      "MaxModelLen": 16384,
      "MaxOutputTokens": 4096,
      "vllm_config": {
        "port": 8002,
        "gpu_device": 1,
        "gpu_memory_utilization": 0.40,
        "tensor_parallel_size": 1,
        "dtype": "bfloat16",
        "trust_remote_code": false,
        "enforce_eager": false
      }
    },
    {
      "Name": "Qwen/Qwen2.5-Coder-14B-Instruct",
      "Description": "The 'Code Specialist'. A highly efficient coding expert that matches GPT-3.5-Turbo and GPT-4o-mini in programming tasks. It is strictly optimized for code generation, debugging, and software engineering workflows. [Benchmarks: HumanEval 84.8%, MBPP 80.2%, McEval 65.9%].",
      "MaxModelLen": 16384,
      "MaxOutputTokens": 4096,
      "vllm_config": {
        "port": 8003,
        "gpu_device": 0,
        "gpu_memory_utilization": 0.30,
        "tensor_parallel_size": 1,
        "dtype": "bfloat16",
        "trust_remote_code": true,
        "enforce_eager": false
      }
    },
    {
      "Name": "meta-llama/Llama-3.1-8B-Instruct",
      "Description": "The 'Instruction Follower'. A robust, standard-sized model for reliable text formatting, summarization, and intermediate processing tasks where 24B+ is overkill. [Benchmarks: MMLU 69.4%, MATH 51.9%, GSM8K 84.5%].",
      "MaxModelLen": 16384,
      "MaxOutputTokens": 4096,
      "vllm_config": {
        "port": 8004,
        "gpu_device": 1,
        "gpu_memory_utilization": 0.20,
        "tensor_parallel_size": 1,
        "dtype": "bfloat16",
        "trust_remote_code": false,
        "enforce_eager": false
      }
    },
    {
      "Name": "meta-llama/Llama-3.2-3B-Instruct",
      "Description": "The 'Flash Router'. An ultra-low latency model used for classification, intent detection, and simple reformatting. Optimized for sub-50ms TTFT to minimize routing overhead. [Benchmarks: Speed >150 tok/s, MMLU 63.4%].",
      "MaxModelLen": 16384,
      "MaxOutputTokens": 4096,
      "vllm_config": {
        "port": 8005,
        "gpu_device": 1,
        "gpu_memory_utilization": 0.10,
        "tensor_parallel_size": 1,
        "dtype": "bfloat16",
        "trust_remote_code": false,
        "enforce_eager": false
      }
    }
  ],
  "model_base_urls": {
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B": "http://127.0.0.1:8001/v1",
    "mistralai/Mistral-Small-24B-Instruct-2501": "http://127.0.0.1:8002/v1",
    "Qwen/Qwen2.5-Coder-14B-Instruct": "http://127.0.0.1:8003/v1",
    "meta-llama/Llama-3.1-8B-Instruct": "http://127.0.0.1:8004/v1",
    "meta-llama/Llama-3.2-3B-Instruct": "http://127.0.0.1:8005/v1"
  }
}